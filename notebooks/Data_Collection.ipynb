{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twiiter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterAPI_data(keyword,tweet_count):\n",
    "    CONSUMER_KEY = \"rYIrIgoJaKS8A5YqCAAVtbkz9\"  \n",
    "    CONSUMER_SECRET = \"wmRnDK0QCOSt3k8JsDTDAYoClSRr0IUTTTNZDd7PkEVpJ1FaHd\"  \n",
    "    ACCESS_TOKEN_KEY = \"1269476398042738688-KspldUGyYSoB1GwCk0yx8TLnjTMC45\"  \n",
    "    ACCESS_TOKEN_SECRET = \"KoUvNsJpGBcvj7gR1YfYFLMgJRuWD3n87wBDYOW00SdLk\"  \n",
    "    api = twitter.Api(  \n",
    "        consumer_key=CONSUMER_KEY,  \n",
    "        consumer_secret=CONSUMER_SECRET,  \n",
    "        access_token_key=ACCESS_TOKEN_KEY,  \n",
    "        access_token_secret=ACCESS_TOKEN_SECRET,  \n",
    "    )  \n",
    "    # The search term you want to find\n",
    "    query = keyword\n",
    "    # Language code (follows ISO 639-1 standards)\n",
    "    language = \"en\"\n",
    "\n",
    "    # Calling the user_timeline function with our parameters\n",
    "    results = api.GetSearch(term=query, lang=language,count=tweet_count)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twiiter Data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Twitter_GitHub_data(file_name):\n",
    "    tweet_data = pd.read_csv(file_name)\n",
    "    return tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseText(text):\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    output=re.findall('>([^<]+)<',text)\n",
    "    final_output=\"\".join(output)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping on FOX News "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_FOX_news(chrome_driver_path,base_url,n_news):\n",
    "    browser = webdriver.Chrome(chrome_driver_path)\n",
    "    browser.get(base_url)\n",
    "    time.sleep(2)\n",
    "    n_page = int(n_news/10)-1\n",
    "    for i in range(0,n_page):\n",
    "        webElem5 = browser.find_element_by_xpath('//*/div[@class=\"button load-more js-load-more\"]')\n",
    "        webElem5.click()\n",
    "        time.sleep(2)\n",
    "    elem1 = browser.find_element_by_xpath(\"//*/div[@class = 'content article-list']\")\n",
    "    links = elem1.find_elements_by_tag_name(\"a\")\n",
    "    desired_links =[]\n",
    "    for link in links:\n",
    "        hyperlink = link.get_attribute(\"href\")\n",
    "        if hyperlink not in desired_links and \"video\" not in hyperlink:\n",
    "            desired_links.append(hyperlink)\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    article_list = []\n",
    "    category_list = []\n",
    "    published_date_list =[]\n",
    "    title_list = []\n",
    "    for url in desired_links:\n",
    "        html_page = requests.get(url)\n",
    "        hp = html_page.text\n",
    "        soup = bs4.BeautifulSoup(hp)\n",
    "        category =url.split(\"/\")[3]\n",
    "        category_list.append(category)\n",
    "        meta2 = soup.select('meta[name=\"dc.date\"]')\n",
    "        published_date = meta2[0].get(\"content\")\n",
    "        published_date_list.append(published_date)\n",
    "        meta3 = soup.select('meta[name=\"dc.title\"]')\n",
    "        title = meta3[0].get(\"content\")\n",
    "        title_list.append(title)\n",
    "        paragraph =soup.find_all(\"p\")\n",
    "        article_content = []\n",
    "        for para in paragraph:\n",
    "            para_class = para.get(\"class\")\n",
    "            text =parseText(str(para))\n",
    "            if para_class is None:\n",
    "                if text !=\"\" and \"FOX\" not in text.upper():\n",
    "                    article_content.append(text)\n",
    "            elif para_class[0] == 'speakable' and \"FOX\" not in text.upper():\n",
    "                if text !=\"\":\n",
    "                    article_content.append(text)\n",
    "        article_full = \"\\n\".join(article_content)\n",
    "        article_list.append(article_full)\n",
    "    merged_data ={\"Published_Date\" :published_date_list,\n",
    "             \"News Source\":[\"Fox News\"]*len(article_list),\n",
    "             \"URL\":desired_links,\n",
    "             \"Title\":title_list,\n",
    "             \"Category\":category_list,\n",
    "             \"Content\":article_list}\n",
    "    merged_df= pd.DataFrame(merged_data)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping on NY Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NYTimes_news(chrome_driver_path,base_url,n_news):\n",
    "    browser = webdriver.Chrome(chrome_driver_path)\n",
    "    browser.get(base_url)\n",
    "    time.sleep(2)\n",
    "    n_page = int(n_news/10)-1\n",
    "    for i in range(0,n_page):\n",
    "        webElem5 = browser.find_element_by_xpath('//*/button[@data-testid=\"search-show-more-button\"]')\n",
    "        webElem5.click()\n",
    "        time.sleep(2)\n",
    "    elem1 = browser.find_elements_by_xpath(\"//*/li[@data-testid = 'search-bodega-result']\")\n",
    "    desired_links =[]\n",
    "    for elem in elem1:\n",
    "        link = elem.find_element_by_tag_name(\"a\")\n",
    "        hyperlink = link.get_attribute(\"href\")\n",
    "        desired_links.append(hyperlink)\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    article_list = []\n",
    "    category_list = []\n",
    "    published_date_list =[]\n",
    "    title_list = []\n",
    "    for url in desired_links:\n",
    "        html_page = requests.get(url)\n",
    "        hp = html_page.text\n",
    "        soup = bs4.BeautifulSoup(hp)\n",
    "        meta1 = soup.select('meta[name=\"CG\"]')\n",
    "        category = meta1[0].get(\"content\")\n",
    "        category_list.append(category)\n",
    "        meta2 = soup.select('meta[name=\"pdate\"]')\n",
    "        text = meta2[0].get(\"content\")\n",
    "        published_date =text[0:4]+\"-\"+text[4:6]+\"-\"+text[6:8]\n",
    "        published_date_list.append(published_date)\n",
    "        meta3 = soup.select('meta[property=\"og:title\"]')\n",
    "        title = meta3[0].get(\"content\")\n",
    "        title_list.append(title)\n",
    "        paragraph =soup.find_all(\"p\")\n",
    "        article_content = []\n",
    "        for para in paragraph:\n",
    "            para_class = para.get(\"class\")\n",
    "            text =parseText(str(para))\n",
    "            if para.string not in ['Advertisement','Supported by'] and text !=\"\":\n",
    "                article_content.append(text)\n",
    "        article_full = \"\\n\".join(article_content)\n",
    "        article_list.append(article_full)\n",
    "    merged_data ={\"Published_Date\" :published_date_list,\n",
    "         \"News Source\":[\"NY Times\"]*len(article_list),\n",
    "         \"URL\":desired_links,\n",
    "         \"Title\":title_list,\n",
    "         \"Category\":category_list,\n",
    "         \"Content\":article_list}\n",
    "    merged_df= pd.DataFrame(merged_data)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrapping on CNN news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CNN_news(chrome_driver_path,base_url,n_news):\n",
    "    browser = webdriver.Chrome(chrome_driver_path)\n",
    "    browser.get(base_url)\n",
    "    time.sleep(2)\n",
    "    desired_links =[]\n",
    "    i=1\n",
    "    while len(desired_links)<n_news:\n",
    "        url= 'https://edition.cnn.com/search?q=coronavirus&size=10&from='+str((i-1)*10)+'&page='+str(i)\n",
    "        i=i+1\n",
    "        browser.get(url)\n",
    "        time.sleep(2)\n",
    "        elem1 = browser.find_elements_by_xpath(\"//*/div[@class = 'cnn-search__result cnn-search__result--article']\")\n",
    "        for elem in elem1:\n",
    "            link = elem.find_element_by_tag_name(\"a\")\n",
    "            hyperlink = link.get_attribute(\"href\")\n",
    "            if len(desired_links) <n_news:\n",
    "                desired_links.append(hyperlink)\n",
    "    browser.close()\n",
    "    browser.quit()\n",
    "    article_list = []\n",
    "    category_list = []\n",
    "    published_date_list =[]\n",
    "    title_list = []\n",
    "    for url in desired_links:\n",
    "        html_page = requests.get(url)\n",
    "        hp = html_page.text\n",
    "        soup = bs4.BeautifulSoup(hp)\n",
    "        meta1 = soup.select('meta[name=\"section\"]')\n",
    "        if len(meta1)==0:\n",
    "            meta1 = soup.select('meta[name=\"meta-section\"]')\n",
    "        category = meta1[0].get(\"content\")\n",
    "        category_list.append(category)\n",
    "        meta2 = soup.select('meta[name=\"pubdate\"]')\n",
    "        if len(meta2)==0:\n",
    "            meta2 = soup.select('meta[property=\"article:published_time\"]')\n",
    "        text = meta2[0].get(\"content\")\n",
    "        published_date =text[0:10]\n",
    "        published_date_list.append(published_date)\n",
    "        meta3 = soup.select('meta[property=\"og:title\"]')\n",
    "        title = meta3[0].get(\"content\")\n",
    "        title_list.append(title)\n",
    "        paragraph =soup.find_all(\"div\",{'class':'zn-body__paragraph'})\n",
    "        article_content = []\n",
    "        for para in paragraph:\n",
    "            text =parseText(str(para))\n",
    "            if text !=\"\":\n",
    "                article_content.append(text)\n",
    "        article_full = \"\\n\".join(article_content)\n",
    "        article_list.append(article_full)\n",
    "    merged_data ={\"Published_Date\" :published_date_list,\n",
    "             \"News Source\":[\"CNN\"]*len(article_list),\n",
    "             \"URL\":desired_links,\n",
    "             \"Title\":title_list,\n",
    "             \"Category\":category_list,\n",
    "             \"Content\":article_list}\n",
    "    merged_df= pd.DataFrame(merged_data)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_data(article_list,folder_name,file_prefix):\n",
    "    i=1\n",
    "    for article in article_list:\n",
    "        f = open(folder_name+\"/\"+file_prefix+\"_\"+str(i)+\".txt\",\"w\",encoding=\"utf-8\") \n",
    "        f.write(article)\n",
    "        f.close()\n",
    "        i+=1 \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_data2(article_list,folder_name,file_prefix):\n",
    "    i=1\n",
    "    for article in article_list:\n",
    "        f = open(folder_name+\"/\"+file_prefix+\"_\"+str(i)+\".txt\",\"w\",encoding=\"utf-8\") \n",
    "        f.write(article)\n",
    "        f.close()\n",
    "        i+=1 \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_data(folder_name,file_prefix):\n",
    "    article_list=[]\n",
    "    for filename in os.listdir(folder_name):\n",
    "        if filename.startswith(file_prefix+\"_\"):\n",
    "            f = open(folder_name+\"/\"+filename,\"r\",encoding=\"utf-8\") \n",
    "            article = f.read()\n",
    "            article_list.append(article)\n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome_driver_path = r'C:\\Users\\admin\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "#base_url = 'https://www.foxnews.com/category/health/infectious-disease/coronavirus'\n",
    "#n_news = 100\n",
    "#data =get_FOX_news(chrome_driver_path,base_url,n_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome_driver_path = r'C:\\Users\\admin\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "#base_url = 'https://www.nytimes.com/search?query=coronavirus'\n",
    "#n_news = 100\n",
    "#data =get_NYTimes_news(chrome_driver_path,base_url,n_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome_driver_path = r'C:\\Users\\admin\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "#base_url = 'https://edition.cnn.com/search?q=coronavirus&size=10&from=0&page=1'\n",
    "#n_news = 100\n",
    "#data =get_CNN_news(chrome_driver_path,base_url,n_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2 =pd.read_pickle(\"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published_Date</th>\n",
       "      <th>News Source</th>\n",
       "      <th>URL</th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/world/live-news/coronaviru...</td>\n",
       "      <td>Coronavirus update: Latest news from around th...</td>\n",
       "      <td>world</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/world/live-news/coronaviru...</td>\n",
       "      <td>August 22 coronavirus news</td>\n",
       "      <td>world</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/23/us/california-w...</td>\n",
       "      <td>Nearly 1 million acres are burning due to wild...</td>\n",
       "      <td>us</td>\n",
       "      <td>Hundreds of fires were started by lightning, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/23/tech/algorithms...</td>\n",
       "      <td>Algorithms promised efficiency. But they've wo...</td>\n",
       "      <td>tech</td>\n",
       "      <td>The 18-year-old, whose full name CNN is not di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/health/us-coron...</td>\n",
       "      <td>Nearly 70,000 lives could be saved in the next...</td>\n",
       "      <td>health</td>\n",
       "      <td>The projection by the University of Washington...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/us/california-w...</td>\n",
       "      <td>California fires: Firefighters say they're str...</td>\n",
       "      <td>us</td>\n",
       "      <td>Hundreds of active fires, including more than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/asia/chinas-com...</td>\n",
       "      <td>China's Communist Party is a threat to the wor...</td>\n",
       "      <td>asia</td>\n",
       "      <td>More recently, she caused a stir with a spate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/opinions/covid-...</td>\n",
       "      <td>Opinion: The sucker punch to America's Covid f...</td>\n",
       "      <td>opinions</td>\n",
       "      <td>After three months of a relentless cycle of br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/politics/postal...</td>\n",
       "      <td>House approves bill to send $25 billion to Pos...</td>\n",
       "      <td>politics</td>\n",
       "      <td>The bill passed 257-150, largely along party l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>CNN</td>\n",
       "      <td>https://www.cnn.com/2020/08/22/politics/biden-...</td>\n",
       "      <td>Biden says he would shut down US to stop coron...</td>\n",
       "      <td>politics</td>\n",
       "      <td>\"I would shut it down. I would listen to the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Published_Date News Source  \\\n",
       "0      2020-08-23         CNN   \n",
       "1      2020-08-22         CNN   \n",
       "2      2020-08-23         CNN   \n",
       "3      2020-08-23         CNN   \n",
       "4      2020-08-22         CNN   \n",
       "..            ...         ...   \n",
       "95     2020-08-22         CNN   \n",
       "96     2020-08-23         CNN   \n",
       "97     2020-08-22         CNN   \n",
       "98     2020-08-22         CNN   \n",
       "99     2020-08-22         CNN   \n",
       "\n",
       "                                                  URL  \\\n",
       "0   https://www.cnn.com/world/live-news/coronaviru...   \n",
       "1   https://www.cnn.com/world/live-news/coronaviru...   \n",
       "2   https://www.cnn.com/2020/08/23/us/california-w...   \n",
       "3   https://www.cnn.com/2020/08/23/tech/algorithms...   \n",
       "4   https://www.cnn.com/2020/08/22/health/us-coron...   \n",
       "..                                                ...   \n",
       "95  https://www.cnn.com/2020/08/22/us/california-w...   \n",
       "96  https://www.cnn.com/2020/08/22/asia/chinas-com...   \n",
       "97  https://www.cnn.com/2020/08/22/opinions/covid-...   \n",
       "98  https://www.cnn.com/2020/08/22/politics/postal...   \n",
       "99  https://www.cnn.com/2020/08/22/politics/biden-...   \n",
       "\n",
       "                                                Title  Category  \\\n",
       "0   Coronavirus update: Latest news from around th...     world   \n",
       "1                          August 22 coronavirus news     world   \n",
       "2   Nearly 1 million acres are burning due to wild...        us   \n",
       "3   Algorithms promised efficiency. But they've wo...      tech   \n",
       "4   Nearly 70,000 lives could be saved in the next...    health   \n",
       "..                                                ...       ...   \n",
       "95  California fires: Firefighters say they're str...        us   \n",
       "96  China's Communist Party is a threat to the wor...      asia   \n",
       "97  Opinion: The sucker punch to America's Covid f...  opinions   \n",
       "98  House approves bill to send $25 billion to Pos...  politics   \n",
       "99  Biden says he would shut down US to stop coron...  politics   \n",
       "\n",
       "                                              Content  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2   Hundreds of fires were started by lightning, C...  \n",
       "3   The 18-year-old, whose full name CNN is not di...  \n",
       "4   The projection by the University of Washington...  \n",
       "..                                                ...  \n",
       "95  Hundreds of active fires, including more than ...  \n",
       "96  More recently, she caused a stir with a spate ...  \n",
       "97  After three months of a relentless cycle of br...  \n",
       "98  The bill passed 257-150, largely along party l...  \n",
       "99  \"I would shut it down. I would listen to the s...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
